{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '/home/sarth/rootdir/workdir/projects/Paper_Data_Latency/Revised_Statistical_Testing/Figure02'\n",
    "\n",
    "def process(varname = 'NSE', min_max = [-10, 1], verbose = False):\n",
    "    df = pd.read_csv(f'{csv_path}/diag_notdiag_{varname}.csv', index_col=0)\n",
    "\n",
    "    # Quick checks\n",
    "    df = df.fillna(min_max[0]).reset_index(drop=True)\n",
    "    if verbose: \n",
    "        print(\"Number of Rows with NaNs?\")\n",
    "        print(df.isna().sum())\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(\"Counts per region:\")\n",
    "        print(df[\"huc\"].value_counts().sort_index())\n",
    "    df[\"diff\"] = df[\"diagonal\"] - df[\"not_diagonal\"]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    ###############################################################\n",
    "\n",
    "    # One-sided WSR (H1: diagonal > non-diagonal)\n",
    "    w = stats.wilcoxon(df[\"diagonal\"], df[\"not_diagonal\"], alternative=\"greater\", zero_method=\"wilcox\")\n",
    "    if verbose: print({\"Wilcoxon_W\": w.statistic, \"pvalue\": w.pvalue})\n",
    "    results[\"Wilcoxon_W\"] = w.statistic\n",
    "    results[\"Wilcoxon_p\"] = w.pvalue\n",
    "\n",
    "    ###############################################################\n",
    "\n",
    "    # Paired effect sizes: rank-biserial and CLES\n",
    "    wins   = int((df[\"diff\"] > 0).sum())\n",
    "    losses = int((df[\"diff\"] < 0).sum())\n",
    "    ties   = int((df[\"diff\"] == 0).sum())\n",
    "\n",
    "    # Rank-biserial correlation (paired; ignoring ties in denom = common practice)\n",
    "    r_rb = (wins - losses) / (wins + losses) if (wins + losses) > 0 else np.nan\n",
    "\n",
    "    # Common-Language Effect Size: P(diagonal > non-diagonal)\n",
    "    cles = wins / (wins + losses) if (wins + losses) > 0 else np.nan\n",
    "    if verbose: print({\"wins\": wins, \"losses\": losses, \"ties\": ties, \"rank_biserial\": r_rb, \"CLES\": cles})\n",
    "    results[\"wins\"] = wins\n",
    "    results[\"losses\"] = losses\n",
    "    results[\"ties\"] = ties\n",
    "    results[\"rank_biserial\"] = r_rb\n",
    "    results[\"CLES\"] = cles\n",
    "\n",
    "    ###############################################################\n",
    "\n",
    "    rng = np.random.default_rng(42)\n",
    "    H = df[\"huc\"].unique()\n",
    "    B = 5000  # increase if you want tighter CIs; keep an eye on runtime\n",
    "\n",
    "    boot_med = np.empty(B)\n",
    "    for b in range(B):\n",
    "        h_samp = rng.choice(H, size=len(H), replace=True) # sample HUCs w/ replacement\n",
    "        d_b = pd.concat([df.loc[df[\"huc\"]==h, \"diff\"] for h in h_samp])\n",
    "        boot_med[b] = np.median(d_b.to_numpy())\n",
    "\n",
    "    median_diff = float(np.median(df[\"diff\"]))\n",
    "    ci_med = (np.percentile(boot_med, 2.5), np.percentile(boot_med, 97.5))\n",
    "    if verbose: print({\"median_diff\": median_diff, \"median_diff_CI95\": ci_med})\n",
    "    results[\"median_diff\"] = median_diff\n",
    "    results[\"median_diff_CI95\"] = ci_med\n",
    "\n",
    "    ###############################################################\n",
    "\n",
    "    # Per-HUC paired WSR (one-sided)\n",
    "    rows = []\n",
    "    for h, g in df.groupby(\"huc\"):\n",
    "        res = stats.wilcoxon(g[\"diagonal\"], g[\"not_diagonal\"],\n",
    "                            alternative=\"greater\", zero_method=\"wilcox\")\n",
    "        med_delta = float(np.median(g[\"diff\"]))\n",
    "        rows.append({\"huc\": h, \"n\": len(g), \"p\": res.pvalue, \"med_delta\": med_delta})\n",
    "\n",
    "    per_huc = pd.DataFrame(rows).sort_values(\"huc\").reset_index(drop=True)\n",
    "\n",
    "    # BH-FDR within the family of per-HUC tests\n",
    "    per_huc[\"q_BH\"] = multipletests(per_huc[\"p\"].to_numpy(), method=\"fdr_bh\")[1]\n",
    "    if verbose: print(per_huc)\n",
    "\n",
    "    # Equal-HUC Stouffer combine (direction from median difference)\n",
    "    p = per_huc[\"p\"].clip(lower=np.finfo(float).tiny).to_numpy()\n",
    "    sign = np.sign(per_huc[\"med_delta\"].to_numpy())\n",
    "    Zs = norm.isf(p) * sign\n",
    "    w = np.ones_like(Zs)  # equal-HUC weighting (avoid big HUC domination)\n",
    "    Z_comb = (w * Zs).sum() / np.sqrt((w**2).sum())\n",
    "    p_comb = norm.sf(Z_comb)\n",
    "    if verbose: print({\"Stouffer_Z_equalHUC\": Z_comb, \"pvalue\": p_comb})\n",
    "\n",
    "    results[\"per_huc\"] = per_huc\n",
    "    results[\"Stouffer_Z_equalHUC\"] = Z_comb\n",
    "    results[\"Stouffer_p_equalHUC\"] = p_comb\n",
    "\n",
    "    ################################################################\n",
    "\n",
    "    x = df[\"diagonal\"].to_numpy()\n",
    "    y = df[\"not_diagonal\"].to_numpy()\n",
    "\n",
    "    # Brunner–Munzel (SciPy)\n",
    "    bm_two = stats.brunnermunzel(x, y, alternative=\"two-sided\")\n",
    "    bm_greater = stats.brunnermunzel(x, y, alternative=\"greater\")\n",
    "    if verbose: print({\"BM_two_sided\": (bm_two.statistic, bm_two.pvalue), \"BM_greater\": (bm_greater.statistic, bm_greater.pvalue)})\n",
    "\n",
    "    results[\"BM_two_sided\"] = (bm_two.statistic, bm_two.pvalue)\n",
    "    results[\"BM_greater\"] = (bm_greater.statistic, bm_greater.pvalue)\n",
    "\n",
    "    ################################################################\n",
    "\n",
    "    # Cliff's delta via Mann–Whitney U (efficient): δ = 2U/(mn) - 1\n",
    "    def cliffs_delta_via_u(a, b):\n",
    "        u = stats.mannwhitneyu(a, b, alternative=\"two-sided\").statistic\n",
    "        m, n = len(a), len(b)\n",
    "        return (2*u)/(m*n) - 1\n",
    "\n",
    "    delta_hat = cliffs_delta_via_u(x, y)\n",
    "\n",
    "    # HUC-cluster bootstrap CI for δ (fast via U in each bootstrap)\n",
    "    rng = np.random.default_rng(123)\n",
    "    H = df[\"huc\"].unique()\n",
    "    B = 2000\n",
    "    boot_delta = np.empty(B)\n",
    "\n",
    "    for b in range(B):\n",
    "        h_samp = rng.choice(H, size=len(H), replace=True)\n",
    "        df_b = pd.concat([df[df[\"huc\"]==h] for h in h_samp], ignore_index=True)\n",
    "        boot_delta[b] = cliffs_delta_via_u(df_b[\"diagonal\"].to_numpy(),\n",
    "                                        df_b[\"not_diagonal\"].to_numpy())\n",
    "\n",
    "    ci_delta = (np.percentile(boot_delta, 2.5), np.percentile(boot_delta, 97.5))\n",
    "    if verbose: print({\"cliffs_delta\": delta_hat, \"cliffs_delta_CI95\": ci_delta})\n",
    "\n",
    "    results[\"cliffs_delta\"] = delta_hat\n",
    "    results[\"cliffs_delta_CI95\"] = ci_delta\n",
    "\n",
    "    ################################################################\n",
    "\n",
    "    def q1(a): return np.percentile(a, 25.0)\n",
    "\n",
    "    q1_diag = q1(x)\n",
    "    q1_nondiag = q1(y)\n",
    "    q1_shift = q1_diag - q1_nondiag\n",
    "\n",
    "    # Cluster bootstrap CI for Q1 shift\n",
    "    rng = np.random.default_rng(99)\n",
    "    boot_q1 = np.empty(B)\n",
    "    for b in range(B):\n",
    "        h_samp = rng.choice(H, size=len(H), replace=True)\n",
    "        df_b = pd.concat([df[df[\"huc\"]==h] for h in h_samp], ignore_index=True)\n",
    "        boot_q1[b] = q1(df_b[\"diagonal\"]) - q1(df_b[\"not_diagonal\"])\n",
    "\n",
    "    ci_q1 = (np.percentile(boot_q1, 2.5), np.percentile(boot_q1, 97.5))\n",
    "    if verbose: print({\"Q1_shift\": q1_shift, \"Q1_shift_CI95\": ci_q1})\n",
    "\n",
    "    results[\"Q1_shift\"] = q1_shift\n",
    "    results[\"Q1_shift_CI95\"] = ci_q1\n",
    "\n",
    "    ###############################################################\n",
    "\n",
    "    # results = {\n",
    "    #     \"Wilcoxon_W\": w.statistic,\n",
    "    #     \"Wilcoxon_p\": w.pvalue,\n",
    "    #     \"wins\": wins,\n",
    "    #     \"losses\": losses,\n",
    "    #     \"ties\": ties,\n",
    "    #     \"rank_biserial\": r_rb,\n",
    "    #     \"CLES\": cles,\n",
    "    #     \"median_diff\": median_diff,\n",
    "    #     \"median_diff_CI95\": ci_med,\n",
    "    #     \"per_huc\": per_huc,\n",
    "    #     \"Stouffer_Z_equalHUC\": Z_comb,\n",
    "    #     \"Stouffer_p_equalHUC\": p_comb,\n",
    "    #     \"BM_two_sided\": (bm_two.statistic, bm_two.pvalue),\n",
    "    #     \"BM_greater\": (bm_greater.statistic, bm_greater.pvalue),\n",
    "    #     \"cliffs_delta\": delta_hat,\n",
    "    #     \"cliffs_delta_CI95\": ci_delta,\n",
    "    #     \"Q1_shift\": q1_shift,\n",
    "    #     \"Q1_shift_CI95\": ci_q1\n",
    "    # }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Wilcoxon_W': np.float64(74258.5),\n",
       " 'Wilcoxon_p': np.float64(5.525707413019561e-65),\n",
       " 'wins': 383,\n",
       " 'losses': 2,\n",
       " 'ties': 10,\n",
       " 'rank_biserial': 0.9896103896103896,\n",
       " 'CLES': 0.9948051948051948,\n",
       " 'median_diff': 0.4,\n",
       " 'median_diff_CI95': (np.float64(0.36), np.float64(0.43000000000000005)),\n",
       " 'per_huc':     huc   n             p  med_delta          q_BH\n",
       " 0     1  12  4.882812e-04      0.295  7.990057e-04\n",
       " 1     2  39  3.863805e-08      0.480  1.390970e-07\n",
       " 2     3  45  2.583848e-09      0.420  1.550309e-08\n",
       " 3     4  16  7.332615e-04      0.355  1.099892e-03\n",
       " 4     5  26  6.122912e-06      0.280  1.224582e-05\n",
       " 5     6  10  9.765625e-04      0.195  1.352163e-03\n",
       " 6     7  26  1.490116e-08      0.420  6.705523e-08\n",
       " 7     8  10  1.953125e-03      0.195  2.511161e-03\n",
       " 8     9   5  3.125000e-02      0.560  3.515625e-02\n",
       " 9    10  52  2.568169e-10      0.495  2.312860e-09\n",
       " 10   11  26  4.135161e-06      0.365  9.327541e-06\n",
       " 11   12  28  4.145574e-06      0.385  9.327541e-06\n",
       " 12   13   4  6.250000e-02      0.300  6.250000e-02\n",
       " 13   14   5  6.250000e-02      0.310  6.250000e-02\n",
       " 14   15  12  2.441406e-04      0.435  4.394531e-04\n",
       " 15   16   6  3.125000e-02      1.730  3.515625e-02\n",
       " 16   17  52  2.569844e-10      0.425  2.312860e-09\n",
       " 17   18  21  4.768372e-07      0.400  1.430511e-06,\n",
       " 'Stouffer_Z_equalHUC': np.float64(16.527245045377253),\n",
       " 'Stouffer_p_equalHUC': np.float64(1.1678908543027124e-61),\n",
       " 'BM_two_sided': (np.float64(-32.98174042537536),\n",
       "  np.float64(7.262814627014237e-127)),\n",
       " 'BM_greater': (np.float64(-32.98174042537536),\n",
       "  np.float64(3.6314073135071184e-127)),\n",
       " 'cliffs_delta': np.float64(0.79351385995834),\n",
       " 'cliffs_delta_CI95': (np.float64(0.7369878278025049),\n",
       "  np.float64(0.8334195059400338)),\n",
       " 'Q1_shift': np.float64(0.79),\n",
       " 'Q1_shift_CI95': (np.float64(0.71), np.float64(0.8600000000000001))}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nse_results = process(varname='NSE', min_max=[-10, 1], verbose=False)\n",
    "nse_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Wilcoxon_W': np.float64(73242.0),\n",
       " 'Wilcoxon_p': np.float64(2.6241376581169765e-54),\n",
       " 'wins': 362,\n",
       " 'losses': 30,\n",
       " 'ties': 3,\n",
       " 'rank_biserial': 0.8469387755102041,\n",
       " 'CLES': 0.923469387755102,\n",
       " 'median_diff': 0.09000000000000008,\n",
       " 'median_diff_CI95': (np.float64(0.08000000000000007),\n",
       "  np.float64(0.10000000000000009)),\n",
       " 'per_huc':     huc   n             p  med_delta          q_BH\n",
       " 0     1  12  4.882812e-04      0.085  8.789062e-04\n",
       " 1     2  39  3.688295e-08      0.120  1.659733e-07\n",
       " 2     3  45  1.786203e-08      0.120  1.408389e-07\n",
       " 3     4  16  3.575223e-03      0.075  4.950309e-03\n",
       " 4     5  26  3.474452e-04      0.065  6.948904e-04\n",
       " 5     6  10  9.765625e-04      0.100  1.464844e-03\n",
       " 6     7  26  1.810315e-04      0.080  4.073209e-04\n",
       " 7     8  10  1.562500e-02      0.080  1.875000e-02\n",
       " 8     9   5  3.125000e-02      0.080  3.515625e-02\n",
       " 9    10  52  2.347315e-08      0.100  1.408389e-07\n",
       " 10   11  26  6.595688e-05      0.090  2.374448e-04\n",
       " 11   12  28  8.425156e-05      0.085  2.527547e-04\n",
       " 12   13   4  6.250000e-02      0.100  6.250000e-02\n",
       " 13   14   5  6.250000e-02      0.030  6.250000e-02\n",
       " 14   15  12  7.324219e-04      0.050  1.198509e-03\n",
       " 15   16   6  1.562500e-02      0.090  1.875000e-02\n",
       " 16   17  52  1.108114e-09      0.095  1.994605e-08\n",
       " 17   18  21  1.603614e-04      0.130  4.073209e-04,\n",
       " 'Stouffer_Z_equalHUC': np.float64(14.609163290931786),\n",
       " 'Stouffer_p_equalHUC': np.float64(1.2275731508036901e-48),\n",
       " 'BM_two_sided': (np.float64(-13.866630531452579),\n",
       "  np.float64(2.2993665927245405e-37)),\n",
       " 'BM_greater': (np.float64(-13.866630531452579),\n",
       "  np.float64(1.1496832963622703e-37)),\n",
       " 'cliffs_delta': np.float64(0.5030668162153502),\n",
       " 'cliffs_delta_CI95': (np.float64(0.44390474492645987),\n",
       "  np.float64(0.5636876415064925)),\n",
       " 'Q1_shift': np.float64(0.12),\n",
       " 'Q1_shift_CI95': (np.float64(0.09999999999999998), np.float64(0.14))}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_results = process(varname='F1', min_max=[0, 1], verbose=False)\n",
    "f1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel c-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 0) Utility helpers\n",
    "# -------------------------------\n",
    "\n",
    "def p_stars(p):\n",
    "    if p < 1e-3: return '***'\n",
    "    if p < 1e-2: return '**'\n",
    "    if p < 5e-2: return '*'\n",
    "    return 'ns'\n",
    "\n",
    "def fmt_p(p):\n",
    "    if p == 0 or (np.isfinite(p) and p < 1e-300):  # handle underflow\n",
    "        return \"≈0\"\n",
    "    return f\"{p:.2e}\"\n",
    "\n",
    "def cluster_bootstrap_ci(values, clusters, stat_fn=np.median, B=5000, seed=42):\n",
    "    \"\"\"\n",
    "    Block/cluster bootstrap: resample unique cluster labels with replacement,\n",
    "    collect all rows from sampled clusters, compute stat_fn on the pooled vector.\n",
    "    Returns (lo, hi) 95% CI.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    values = np.asarray(values)\n",
    "    clusters = np.asarray(clusters)\n",
    "    H = np.unique(clusters)\n",
    "    boot = np.empty(B, dtype=float)\n",
    "    for b in range(B):\n",
    "        h_samp = rng.choice(H, size=len(H), replace=True)\n",
    "        idx = np.isin(clusters, h_samp)\n",
    "        boot[b] = stat_fn(values[idx])\n",
    "    return (np.nanpercentile(boot, 2.5), np.nanpercentile(boot, 97.5))\n",
    "\n",
    "def cliffs_delta_via_u(a, b):\n",
    "    \"\"\"\n",
    "    Fast Cliff's delta via Mann–Whitney U:\n",
    "    delta = 2U/(mn) - 1\n",
    "    \"\"\"\n",
    "    a = np.asarray(a); b = np.asarray(b)\n",
    "    u = stats.mannwhitneyu(a, b, alternative=\"two-sided\").statistic\n",
    "    m, n = len(a), len(b)\n",
    "    return (2*u)/(m*n) - 1\n",
    "\n",
    "def stouffer_equal_huc(pvals, signs):\n",
    "    \"\"\"\n",
    "    Combine one-sided p-values across HUCs with equal HUC weights.\n",
    "    'signs' derived from direction (e.g., median delta).\n",
    "    \"\"\"\n",
    "    p = np.clip(np.asarray(pvals, dtype=float), np.finfo(float).tiny, 1.0)\n",
    "    Z = norm.isf(p) * np.sign(signs)\n",
    "    w = np.ones_like(Z)\n",
    "    Zc = (w * Z).sum() / np.sqrt((w**2).sum())\n",
    "    pc = norm.sf(Zc)\n",
    "    return Zc, pc\n",
    "\n",
    "def paired_effect_sizes(diff):\n",
    "    \"\"\"\n",
    "    Rank-biserial (paired) and CLES from wins/losses/ties.\n",
    "    \"\"\"\n",
    "    diff = np.asarray(diff)\n",
    "    wins   = int((diff > 0).sum())\n",
    "    losses = int((diff < 0).sum())\n",
    "    ties   = int((diff == 0).sum())\n",
    "    denom = wins + losses\n",
    "    if denom == 0:\n",
    "        r_rb = np.nan\n",
    "        cles = np.nan\n",
    "    else:\n",
    "        r_rb = (wins - losses) / denom\n",
    "        cles = wins / denom\n",
    "    return wins, losses, ties, r_rb, cles\n",
    "\n",
    "# CDF/PDF curve metrics\n",
    "def trapezoid_integral(x, y): return np.trapz(y, x)\n",
    "def ks_distance(cdf_a, cdf_b): return np.max(np.abs(cdf_a - cdf_b))\n",
    "def cvm_distance(x, cdf_a, cdf_b): return trapezoid_integral(x, (cdf_a - cdf_b)**2)\n",
    "def wasserstein1(x, cdf_a, cdf_b): return trapezoid_integral(x, np.abs(cdf_a - cdf_b))\n",
    "def hellinger(x, pdf_a, pdf_b): return np.sqrt(0.5 * trapezoid_integral(x, (np.sqrt(pdf_a) - np.sqrt(pdf_b))**2))\n",
    "\n",
    "def dominance_violations(cdf_cont, cdf_reg):\n",
    "    \"\"\"\n",
    "    For first-order dominance (higher is better), we want F_cont(x) <= F_reg(x) for all x.\n",
    "    Report max positive violation and fraction of violating grid points.\n",
    "    \"\"\"\n",
    "    diff = cdf_cont - cdf_reg\n",
    "    return float(np.max(np.maximum(diff, 0.0))), float(np.mean(diff > 0.0))\n",
    "\n",
    "def invert_cdf(x_grid, cdf, p):\n",
    "    return float(np.interp(p, cdf, x_grid, left=x_grid[0], right=x_grid[-1]))\n",
    "\n",
    "def quantile_shifts_from_cdfs(x_grid, cdf_c, cdf_r, probs=(0.10,0.25,0.50,0.75,0.90)):\n",
    "    out = []\n",
    "    for pr in probs:\n",
    "        qc = invert_cdf(x_grid, cdf_c, pr)\n",
    "        qr = invert_cdf(x_grid, cdf_r, pr)\n",
    "        out.append((pr, qc - qr, qc, qr))\n",
    "    return pd.DataFrame(out, columns=[\"p\",\"DeltaQ\",\"Q_cont\",\"Q_reg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1) Load data & pair basins\n",
    "# -------------------------------\n",
    "csv_path = '/home/sarth/rootdir/workdir/projects/Paper_Data_Latency/Revised_Statistical_Testing/Figure02'\n",
    "cont = pd.read_csv(f\"{csv_path}/continental.csv\")          # columns: huc, NSE, F1\n",
    "reg  = pd.read_csv(f\"{csv_path}/median_regional.csv\")      # columns: huc, NSE, F1\n",
    "\n",
    "cont = cont.rename(columns={\"prediction_huc\": \"huc\"})\n",
    "reg = reg.rename(columns={\"prediction_huc\": \"huc\"})\n",
    "\n",
    "# Fill NaNs in NSE column with -10, and in F1 column with 0\n",
    "cont['NSE'] = cont['NSE'].fillna(-10)\n",
    "reg['NSE'] = reg['NSE'].fillna(-10)\n",
    "cont['F1'] = cont['F1'].fillna(0)\n",
    "reg['F1'] = reg['F1'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paired rows: 395 Unique HUCs: 18\n"
     ]
    }
   ],
   "source": [
    "# Pairing strategy:\n",
    "# If a unique basin identifier exists in both files (e.g., 'basin_id'), merge on it.\n",
    "# Otherwise, we create a within-HUC index and merge on (huc, idx). This assumes\n",
    "# the ordering within each HUC is consistent across both files.\n",
    "\n",
    "if \"basin_id\" in cont.columns and \"basin_id\" in reg.columns:\n",
    "    df = cont.merge(reg, on=[\"basin_id\",\"huc\"], suffixes=(\"_C\", \"_R\"))\n",
    "else:\n",
    "    cont = cont.copy()\n",
    "    reg  = reg.copy()\n",
    "    cont[\"idx_within_huc\"] = cont.groupby(\"huc\").cumcount()\n",
    "    reg[\"idx_within_huc\"]  = reg.groupby(\"huc\").cumcount()\n",
    "    df = cont.merge(reg, on=[\"huc\",\"idx_within_huc\"], suffixes=(\"_C\", \"_R\"))\n",
    "    # If you know files are in identical row order, you could simply join by index instead.\n",
    "\n",
    "# Sanity checks\n",
    "assert df[[\"NSE_C\",\"NSE_R\",\"F1_C\",\"F1_R\",\"huc\"]].notna().all().all(), \"NaNs present after merge\"\n",
    "print(\"Paired rows:\", df.shape[0], \"Unique HUCs:\", df['huc'].nunique())\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Primary paired inference (per metric)\n",
    "# -------------------------------\n",
    "\n",
    "def paired_wilcoxon_block(df, metric_col_C, metric_col_R, huc_col=\"huc\", one_sided=True, B=5000, seed=42):\n",
    "    C = df[metric_col_C].to_numpy()\n",
    "    R = df[metric_col_R].to_numpy()\n",
    "    H = df[huc_col].to_numpy()\n",
    "    diff = C - R\n",
    "\n",
    "    # Wilcoxon signed-rank (one-sided C > R)\n",
    "    res = stats.wilcoxon(C, R, alternative=(\"greater\" if one_sided else \"two-sided\"), zero_method=\"wilcox\")\n",
    "    W, p_w = res.statistic, res.pvalue\n",
    "\n",
    "    # Paired effect sizes\n",
    "    wins, losses, ties, r_rb, cles = paired_effect_sizes(diff)\n",
    "\n",
    "    # Median paired difference + HUC-cluster CI\n",
    "    med_diff = float(np.median(diff))\n",
    "    ci_lo, ci_hi = cluster_bootstrap_ci(diff, clusters=H, stat_fn=np.median, B=B, seed=seed)\n",
    "\n",
    "    return {\n",
    "        \"W\": float(W), \"p_wilcoxon\": float(p_w),\n",
    "        \"wins\": wins, \"losses\": losses, \"ties\": ties,\n",
    "        \"median_diff\": med_diff, \"median_diff_CI95\": (float(ci_lo), float(ci_hi)),\n",
    "        \"r_rb\": float(r_rb), \"CLES\": float(cles),\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Per-HUC paired tests + BH-FDR + Stouffer\n",
    "# -------------------------------\n",
    "\n",
    "def per_huc_paired(df, metric_col_C, metric_col_R, huc_col=\"huc\"):\n",
    "    rows = []\n",
    "    for h, g in df.groupby(huc_col):\n",
    "        if g.shape[0] < 2:\n",
    "            rows.append({\"huc\": h, \"n\": len(g), \"p\": np.nan, \"med_delta\": float(np.median(g[metric_col_C]-g[metric_col_R]))})\n",
    "            continue\n",
    "        res = stats.wilcoxon(g[metric_col_C], g[metric_col_R], alternative=\"greater\", zero_method=\"wilcox\")\n",
    "        rows.append({\"huc\": h, \"n\": len(g), \"p\": float(res.pvalue), \"med_delta\": float(np.median(g[metric_col_C]-g[metric_col_R]))})\n",
    "    per = pd.DataFrame(rows).sort_values(\"huc\").reset_index(drop=True)\n",
    "    # BH-FDR within metric\n",
    "    mask = per[\"p\"].notna()\n",
    "    per.loc[mask, \"q_BH\"] = multipletests(per.loc[mask, \"p\"].to_numpy(), method=\"fdr_bh\")[1]\n",
    "    per[\"q_BH\"] = per[\"q_BH\"].astype(float)\n",
    "    # Stouffer (equal HUC weights)\n",
    "    valid = per[\"p\"].notna()\n",
    "    Zc, pc = stouffer_equal_huc(per.loc[valid, \"p\"].to_numpy(), per.loc[valid, \"med_delta\"].to_numpy())\n",
    "    return per, Zc, pc\n",
    "\n",
    "# -------------------------------\n",
    "# 4) Distributional lens: BM + Cliff's δ + Q1 shift (with cluster CIs)\n",
    "# -------------------------------\n",
    "\n",
    "def distributional_lens(df, metric_col_C, metric_col_R, huc_col=\"huc\", B=5000, seed=123):\n",
    "    C = df[metric_col_C].to_numpy()\n",
    "    R = df[metric_col_R].to_numpy()\n",
    "    H = df[huc_col].to_numpy()\n",
    "\n",
    "    # Brunner–Munzel (one-sided C > R)\n",
    "    bm = stats.brunnermunzel(C, R, alternative=\"greater\")\n",
    "\n",
    "    # Cliff's delta (point estimate)\n",
    "    delta_hat = cliffs_delta_via_u(C, R)\n",
    "\n",
    "    # Cluster CI for delta\n",
    "    rng = np.random.default_rng(seed)\n",
    "    Huniq = np.unique(H)\n",
    "    boot_d = np.empty(B)\n",
    "    for b in range(B):\n",
    "        hs = rng.choice(Huniq, size=len(Huniq), replace=True)\n",
    "        idx = np.isin(H, hs)\n",
    "        boot_d[b] = cliffs_delta_via_u(C[idx], R[idx])\n",
    "    d_lo, d_hi = np.percentile(boot_d, 2.5), np.percentile(boot_d, 97.5)\n",
    "\n",
    "    # Q1 (25th percentile) shift + cluster CI\n",
    "    def q1_shift(valsC, valsR): return np.percentile(valsC, 25.0) - np.percentile(valsR, 25.0)\n",
    "    q1 = q1_shift(C, R)\n",
    "\n",
    "    boot_q1 = np.empty(B)\n",
    "    for b in range(B):\n",
    "        hs = rng.choice(Huniq, size=len(Huniq), replace=True)\n",
    "        idx = np.isin(H, hs)\n",
    "        boot_q1[b] = q1_shift(C[idx], R[idx])\n",
    "    q_lo, q_hi = np.percentile(boot_q1, 2.5), np.percentile(boot_q1, 97.5)\n",
    "\n",
    "    return {\n",
    "        \"bm_stat\": float(bm.statistic), \"bm_p\": float(bm.pvalue),\n",
    "        \"delta\": float(delta_hat), \"delta_CI95\": (float(d_lo), float(d_hi)),\n",
    "        \"Q1_shift\": float(q1), \"Q1_shift_CI95\": (float(q_lo), float(q_hi)),\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 5) CDF/PDF curve metrics (descriptive) from files\n",
    "# -------------------------------\n",
    "\n",
    "def curve_metrics_from_files(nse_curve_csv=\"NSE_pdf_cdf.csv\", f1_curve_csv=\"F1_pdf_cdf.csv\"):\n",
    "    nse_curves = pd.read_csv(nse_curve_csv)\n",
    "    f1_curves  = pd.read_csv(f1_curve_csv)\n",
    "\n",
    "    # NSE\n",
    "    x = nse_curves[\"grid\"].to_numpy()\n",
    "    Fn_c = nse_curves[\"NSE_cdf_continental\"].to_numpy()\n",
    "    Fn_r = nse_curves[\"NSE_cdf_regional\"].to_numpy()\n",
    "    pn_c = nse_curves[\"NSE_pdf_continental\"].to_numpy()\n",
    "    pn_r = nse_curves[\"NSE_pdf_regional\"].to_numpy()\n",
    "\n",
    "    nse_curve = {\n",
    "        \"KS\": ks_distance(Fn_c, Fn_r),\n",
    "        \"CvM\": cvm_distance(x, Fn_c, Fn_r),\n",
    "        \"W1\": wasserstein1(x, Fn_c, Fn_r),\n",
    "        \"Hellinger\": hellinger(x, pn_c, pn_r),\n",
    "        \"Dom.max_violation\": dominance_violations(Fn_c, Fn_r)[0],\n",
    "        \"Dom.frac_violation\": dominance_violations(Fn_c, Fn_r)[1],\n",
    "        \"Qshifts\": quantile_shifts_from_cdfs(x, Fn_c, Fn_r)\n",
    "    }\n",
    "\n",
    "    # F1\n",
    "    x = f1_curves[\"grid\"].to_numpy()\n",
    "    F1_c = f1_curves[\"F1_cdf_continental\"].to_numpy()\n",
    "    F1_r = f1_curves[\"F1_cdf_regional\"].to_numpy()\n",
    "    pf_c = f1_curves[\"F1_pdf_continental\"].to_numpy()\n",
    "    pf_r = f1_curves[\"F1_pdf_regional\"].to_numpy()\n",
    "\n",
    "    f1_curve = {\n",
    "        \"KS\": ks_distance(F1_c, F1_r),\n",
    "        \"CvM\": cvm_distance(x, F1_c, F1_r),\n",
    "        \"W1\": wasserstein1(x, F1_c, F1_r),\n",
    "        \"Hellinger\": hellinger(x, pf_c, pf_r),\n",
    "        \"Dom.max_violation\": dominance_violations(F1_c, F1_r)[0],\n",
    "        \"Dom.frac_violation\": dominance_violations(F1_c, F1_r)[1],\n",
    "        \"Qshifts\": quantile_shifts_from_cdfs(x, F1_c, F1_r)\n",
    "    }\n",
    "\n",
    "    return nse_curve, f1_curve\n",
    "\n",
    "# -------------------------------\n",
    "# 6) Run pipeline for NSE and F1\n",
    "# -------------------------------\n",
    "\n",
    "def run_metric(df, metric, B_ci=5000):\n",
    "    C_col = f\"{metric}_C\"\n",
    "    R_col = f\"{metric}_R\"\n",
    "\n",
    "    # Paired inference\n",
    "    paired = paired_wilcoxon_block(df, C_col, R_col, huc_col=\"huc\", one_sided=True, B=B_ci)\n",
    "\n",
    "    # Per-HUC drilldown\n",
    "    per_huc, Zc, pc = per_huc_paired(df, C_col, R_col, huc_col=\"huc\")\n",
    "    hucs_sig = int((per_huc[\"q_BH\"] < 0.05).sum())\n",
    "\n",
    "    # Distributional lens\n",
    "    dist = distributional_lens(df, C_col, R_col, huc_col=\"huc\", B=B_ci)\n",
    "\n",
    "    # Assemble summary row\n",
    "    summary = {\n",
    "        \"Metric\": metric,\n",
    "        \"N (basins)\": df.shape[0],\n",
    "        \"Wins/Losses/Ties\": f'{paired[\"wins\"]} / {paired[\"losses\"]} / {paired[\"ties\"]}',\n",
    "        \"Median Δ [95% CI]\": f'{paired[\"median_diff\"]:+.3f} [{paired[\"median_diff_CI95\"][0]:.3f}, {paired[\"median_diff_CI95\"][1]:.3f}]',\n",
    "        \"Wilcoxon W, p\": f'{paired[\"W\"]:,.1f}; p={fmt_p(paired[\"p_wilcoxon\"])} {p_stars(paired[\"p_wilcoxon\"])}',\n",
    "        \"Rank-biserial r_rb\": f'{paired[\"r_rb\"]:.3f}',\n",
    "        \"CLES\": f'{paired[\"CLES\"]:.3f}',\n",
    "        \"BM p (one-sided)\": f'{dist[\"bm_stat\"]} p={fmt_p(dist[\"bm_p\"])} {p_stars(dist[\"bm_p\"])}',\n",
    "        \"Cliff’s δ [95% CI]\": f'{dist[\"delta\"]:.3f} [{dist[\"delta_CI95\"][0]:.3f}, {dist[\"delta_CI95\"][1]:.3f}]',\n",
    "        \"Q1 shift [95% CI]\": f'{dist[\"Q1_shift\"]:+.3f} [{dist[\"Q1_shift_CI95\"][0]:.3f}, {dist[\"Q1_shift_CI95\"][1]:.3f}]',\n",
    "        \"HUCs q<0.05 / 18\": f'{hucs_sig} / {df[\"huc\"].nunique()}',\n",
    "        \"Stouffer Z, p\": f'{Zc:.2f}; p={fmt_p(pc)} {p_stars(pc)}',\n",
    "    }\n",
    "    return summary, per_huc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute\n",
    "summary_nse, per_huc_nse = run_metric(df, \"NSE\", B_ci=3000)\n",
    "summary_f1,  per_huc_f1  = run_metric(df, \"F1\",  B_ci=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Main summary (paired + distributional) ===\n",
      "| Metric   |   N (basins) | Wins/Losses/Ties   | Median Δ [95% CI]     | Wilcoxon W, p            |   Rank-biserial r_rb |   CLES | BM p (one-sided)                    | Cliff’s δ [95% CI]   | Q1 shift [95% CI]     | HUCs q<0.05 / 18   | Stouffer Z, p         |\n",
      "|:---------|-------------:|:-------------------|:----------------------|:-------------------------|---------------------:|-------:|:------------------------------------|:---------------------|:----------------------|:-------------------|:----------------------|\n",
      "| NSE      |          395 | 386 / 9 / 0        | +0.400 [0.386, 0.416] | 74,769.0; p=6.74e-56 *** |                0.954 |  0.977 | -33.669698809588816 p=2.13e-129 *** | 0.800 [0.771, 0.828] | +0.754 [0.697, 0.809] | 16 / 18            | 16.31; p=4.29e-60 *** |\n",
      "| F1       |          395 | 363 / 31 / 1       | +0.094 [0.087, 0.101] | 73,713.0; p=9.93e-54 *** |                0.843 |  0.921 | -13.01646020684048 p=6.52e-34 ***   | 0.481 [0.435, 0.525] | +0.119 [0.090, 0.131] | 14 / 18            | 15.41; p=7.31e-54 *** |\n"
     ]
    }
   ],
   "source": [
    "summary_tbl = pd.DataFrame([summary_nse, summary_f1])\n",
    "print(\"\\n=== Main summary (paired + distributional) ===\")\n",
    "print(summary_tbl.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Per-HUC (NSE) with BH-FDR ===\n",
      "|   huc |   n |           p |   med_delta |        q_BH |\n",
      "|------:|----:|------------:|------------:|------------:|\n",
      "|     1 |  12 | 0.0170898   |    0.313206 | 0.0205078   |\n",
      "|     2 |  39 | 1.37952e-08 |    0.454908 | 4.47035e-08 |\n",
      "|     3 |  45 | 2.84217e-14 |    0.421025 | 5.11591e-13 |\n",
      "|     4 |  16 | 0.00257874  |    0.426015 | 0.00357056  |\n",
      "|     5 |  26 | 1.34706e-05 |    0.370727 | 3.0309e-05  |\n",
      "|     6 |  10 | 0.000976562 |    0.294183 | 0.00146484  |\n",
      "|     7 |  26 | 1.49012e-08 |    0.428108 | 4.47035e-08 |\n",
      "|     8 |  10 | 0.000976562 |    0.256916 | 0.00146484  |\n",
      "|     9 |   5 | 0.03125     |    0.606978 | 0.0351562   |\n",
      "|    10 |  52 | 3.29395e-09 |    0.417784 | 1.97637e-08 |\n",
      "|    11 |  26 | 1.49012e-08 |    0.32731  | 4.47035e-08 |\n",
      "|    12 |  28 | 0.000108805 |    0.386444 | 0.000217609 |\n",
      "|    13 |   4 | 0.0625      |    0.353254 | 0.0661765   |\n",
      "|    14 |   5 | 0.3125      |    0.310917 | 0.3125      |\n",
      "|    15 |  12 | 0.000244141 |    0.507713 | 0.000439453 |\n",
      "|    16 |   6 | 0.015625    |    1.46434  | 0.0200893   |\n",
      "|    17 |  52 | 3.29395e-09 |    0.411206 | 1.97637e-08 |\n",
      "|    18 |  21 | 4.76837e-07 |    0.398459 | 1.22615e-06 |\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Per-HUC (NSE) with BH-FDR ===\")\n",
    "print(per_huc_nse.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Per-HUC (F1) with BH-FDR ===\n",
      "|   huc |   n |           p |   med_delta |        q_BH |\n",
      "|------:|----:|------------:|------------:|------------:|\n",
      "|     1 |  12 | 0.000488281 |   0.0847923 | 0.000878906 |\n",
      "|     2 |  39 | 2.49202e-10 |   0.11823   | 2.24281e-09 |\n",
      "|     3 |  45 | 4.80327e-12 |   0.12122   | 8.64588e-11 |\n",
      "|     4 |  16 | 0.0288391   |   0.0890908 | 0.0370789   |\n",
      "|     5 |  26 | 6.29425e-05 |   0.0859306 | 0.000141621 |\n",
      "|     6 |  10 | 0.00488281  |   0.0884298 | 0.00676082  |\n",
      "|     7 |  26 | 4.47035e-08 |   0.0950203 | 1.82588e-07 |\n",
      "|     8 |  10 | 0.000976562 |   0.0884488 | 0.00146484  |\n",
      "|     9 |   5 | 0.21875     |   0.0320068 | 0.21875     |\n",
      "|    10 |  52 | 5.07189e-08 |   0.0824341 | 1.82588e-07 |\n",
      "|    11 |  26 | 1.04308e-07 |   0.103985  | 3.12924e-07 |\n",
      "|    12 |  28 | 0.000212483 |   0.0902088 | 0.000424966 |\n",
      "|    13 |   4 | 0.0625      |   0.113079  | 0.0661765   |\n",
      "|    14 |   5 | 0.0625      |   0.0833858 | 0.0661765   |\n",
      "|    15 |  12 | 0.000732422 |   0.0753115 | 0.00119851  |\n",
      "|    16 |   6 | 0.046875    |   0.074904  | 0.05625     |\n",
      "|    17 |  52 | 1.993e-08   |   0.099499  | 1.1958e-07  |\n",
      "|    18 |  21 | 2.6226e-05  |   0.0962903 | 6.74384e-05 |\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Per-HUC (F1) with BH-FDR ===\")\n",
    "print(per_huc_f1.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Curve distances (descriptive) ===\n",
      "| Metric   |       KS |       CvM |        W1 |   Hellinger |   Dom.max_violation |   Dom.frac_violation |\n",
      "|:---------|---------:|----------:|----------:|------------:|--------------------:|---------------------:|\n",
      "| NSE      | 0.624051 | 0.21472   | 0.567856  |         nan |          0          |                 0    |\n",
      "| F1       | 0.421519 | 0.0256405 | 0.0999489 |         nan |          0.00253165 |                 0.06 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73250/1783315693.py:73: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  def trapezoid_integral(x, y): return np.trapz(y, x)\n",
      "/tmp/ipykernel_73250/1783315693.py:77: RuntimeWarning: invalid value encountered in sqrt\n",
      "  def hellinger(x, pdf_a, pdf_b): return np.sqrt(0.5 * trapezoid_integral(x, (np.sqrt(pdf_a) - np.sqrt(pdf_b))**2))\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 7) Curve-level distances (descriptive; aligns with your PDF/CDF panels)\n",
    "# -------------------------------\n",
    "nse_curve, f1_curve = curve_metrics_from_files(f\"{csv_path}/NSE_cdf_pdf.csv\", f\"{csv_path}/F1_cdf_pdf.csv\")\n",
    "\n",
    "curve_tbl = pd.DataFrame([\n",
    "    {\"Metric\": \"NSE\", \"KS\": nse_curve[\"KS\"], \"CvM\": nse_curve[\"CvM\"], \"W1\": nse_curve[\"W1\"],\n",
    "     \"Hellinger\": nse_curve[\"Hellinger\"], \"Dom.max_violation\": nse_curve[\"Dom.max_violation\"],\n",
    "     \"Dom.frac_violation\": nse_curve[\"Dom.frac_violation\"]},\n",
    "    {\"Metric\": \"F1\", \"KS\": f1_curve[\"KS\"], \"CvM\": f1_curve[\"CvM\"], \"W1\": f1_curve[\"W1\"],\n",
    "     \"Hellinger\": f1_curve[\"Hellinger\"], \"Dom.max_violation\": f1_curve[\"Dom.max_violation\"],\n",
    "     \"Dom.frac_violation\": f1_curve[\"Dom.frac_violation\"]},\n",
    "])\n",
    "print(\"\\n=== Curve distances (descriptive) ===\")\n",
    "print(curve_tbl.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
