{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Utilities\n",
    "# ------------------------\n",
    "\n",
    "def _make_bins(frac, bin_size=0.25):\n",
    "    # 0-1 inclusive end handling\n",
    "    idx = np.floor(np.clip(frac - 1e-12, 0, 1) / bin_size).astype(int)\n",
    "    idx[idx >= int(1/bin_size)] = int(1/bin_size) - 1\n",
    "    return idx + 1  # bins 1..4\n",
    "\n",
    "def cliffs_delta(a, b):\n",
    "    # fast via Mann-Whitney U: delta = 2U/(mn) - 1\n",
    "    u = stats.mannwhitneyu(a, b, alternative=\"two-sided\").statistic\n",
    "    m, n = len(a), len(b)\n",
    "    return (2*u)/(m*n) - 1\n",
    "\n",
    "def common_language_effect(a, b):\n",
    "    # estimate P(A>B) using ranks (Mann-Whitney U)\n",
    "    u = stats.mannwhitneyu(a, b, alternative=\"two-sided\").statistic\n",
    "    m, n = len(a), len(b)\n",
    "    return u / (m*n)\n",
    "\n",
    "def cluster_bootstrap_ci(df, group_col, vec1, vec2, stat_func, B=4000, seed=123):\n",
    "    \"\"\"\n",
    "    Cluster bootstrap CI for a contrast statistic between two groups.\n",
    "    df -> a DataFrame containing group_col and both vec1, vec2 columns.\n",
    "    stat_func must accept two 1D arrays (x, y) -> float (e.g., quantile diff).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    clusters = df[group_col].unique()\n",
    "    boot = np.empty(B)\n",
    "    for b in range(B):\n",
    "        samp = rng.choice(clusters, size=len(clusters), replace=True)\n",
    "        df_b = pd.concat([df[df[group_col]==g] for g in samp], ignore_index=True)\n",
    "        x = df_b.loc[df_b[\"bin\"].isin([1,2,3]), vec1].dropna().to_numpy()\n",
    "        y = df_b.loc[df_b[\"bin\"].isin([4]), vec2].dropna().to_numpy()\n",
    "        boot[b] = stat_func(x, y)\n",
    "    return (np.percentile(boot, 2.5), np.percentile(boot, 97.5))\n",
    "\n",
    "def quantile_diff_func(tau):\n",
    "    def f(x, y):\n",
    "        return np.percentile(x, tau*100.0) - np.percentile(y, tau*100.0)\n",
    "    return f\n",
    "\n",
    "# Simple Dunn pairwise (uses pooled ranks then MW z-scores); BH outside.\n",
    "def dunn_pairwise(groups, values):\n",
    "    \"\"\"\n",
    "    groups: array of group labels (e.g., 1..4)\n",
    "    values: array of metric values (e.g., NSE)\n",
    "    Returns a DataFrame of pairwise z and unadjusted p-values (two-sided).\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"g\": groups, \"x\": values}).dropna()\n",
    "    # rank all\n",
    "    df[\"r\"] = stats.rankdata(df[\"x\"])\n",
    "    N = len(df)\n",
    "    grouped = df.groupby(\"g\")\n",
    "    stats_by_g = grouped.agg(n=(\"x\", \"size\"), R=(\"r\", \"sum\"))\n",
    "    pairs = []\n",
    "    glabels = stats_by_g.index.to_list()\n",
    "    for i in range(len(glabels)):\n",
    "        for j in range(i+1, len(glabels)):\n",
    "            gi, gj = glabels[i], glabels[j]\n",
    "            ni, nj = stats_by_g.loc[gi, \"n\"], stats_by_g.loc[gj, \"n\"]\n",
    "            Ri, Rj = stats_by_g.loc[gi, \"R\"], stats_by_g.loc[gj, \"R\"]\n",
    "            # mean rank per group\n",
    "            Ti = Ri/ni; Tj = Rj/nj\n",
    "            # Dunn z (no tie correction, usually fine for large N)\n",
    "            S = np.sqrt((N*(N+1)/12.0)*(1/ni + 1/nj))\n",
    "            z = (Ti - Tj)/S\n",
    "            p = 2*stats.norm.sf(abs(z))\n",
    "            pairs.append({\"g1\": gi, \"g2\": gj, \"z\": z, \"p\": p})\n",
    "    return pd.DataFrame(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Main analysis function\n",
    "# ------------------------\n",
    "\n",
    "def analyze_no_flow_effect(\n",
    "    csv_file,\n",
    "    metric=\"NSE\",                      # or \"F1\"\n",
    "    noflow_col=\"no_flow_threshold\",    # or \"no_flow_strict\"\n",
    "    bin_size=0.25,\n",
    "    alpha=0.05,\n",
    "    B_boot=4000,\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Produces:\n",
    "      - Global Kruskal-Wallis across 4 bins\n",
    "      - Spearman rho (bin index vs metric) with cluster-boot CI\n",
    "      - KW among bins 1-3\n",
    "      - Dunn pairwise among bins 1-3 (BH-FDR)\n",
    "      - R(=bins1-3) vs bin4: Brunner–Munzel (one-sided), Cliff's delta, CLES (+ cluster-boot CI for delta)\n",
    "      - Quantile shift R vs bin4 with cluster-boot CIs for tau in {0.1,0.25,0.5,0.75,0.9}\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df[[ \"prediction_huc\", metric, noflow_col ]].dropna().copy()\n",
    "    df[\"bin\"] = _make_bins(df[noflow_col].to_numpy(), bin_size=bin_size)\n",
    "\n",
    "    # ---------------- Global: KW across 4 bins ----------------\n",
    "    groups = [g[metric].dropna().to_numpy() for _, g in df.groupby(\"bin\")]\n",
    "    kw_stat, kw_p = stats.kruskal(*groups)\n",
    "\n",
    "    # Trend: Spearman (bin index vs metric), cluster-boot CI\n",
    "    # Build per-row bin index\n",
    "    spearman_rho, spearman_p = stats.spearmanr(df[\"bin\"], df[metric], nan_policy=\"omit\")\n",
    "    # Bootstrap CI for rho (cluster by HUC)\n",
    "    def stat_rho(xdf):\n",
    "        return stats.spearmanr(xdf[\"bin\"], xdf[metric], nan_policy=\"omit\").correlation\n",
    "    rng = np.random.default_rng(42)\n",
    "    H = df[\"prediction_huc\"].unique()\n",
    "    boot_rho = np.empty(B_boot)\n",
    "    for b in range(B_boot):\n",
    "        samp = rng.choice(H, size=len(H), replace=True)\n",
    "        xdf = pd.concat([df[df[\"prediction_huc\"]==h] for h in samp], ignore_index=True)\n",
    "        boot_rho[b] = stat_rho(xdf)\n",
    "    rho_ci = (np.percentile(boot_rho, 2.5), np.percentile(boot_rho, 97.5))\n",
    "\n",
    "    # ---------------- Similarity among bins 1-3 ----------------\n",
    "    df_123 = df[df[\"bin\"].isin([1,2,3])]\n",
    "    groups_123 = [g[metric].dropna().to_numpy() for _, g in df_123.groupby(\"bin\")]\n",
    "    kw123_stat, kw123_p = stats.kruskal(*groups_123)\n",
    "\n",
    "    # Optional: Dunn pairwise among 1-3 with BH-FDR\n",
    "    dunn_123 = dunn_pairwise(df_123[\"bin\"].to_numpy(), df_123[metric].to_numpy())\n",
    "    dunn_123[\"q_BH\"] = multipletests(dunn_123[\"p\"].to_numpy(), method=\"fdr_bh\")[1]\n",
    "\n",
    "    # ---------------- Bin4 vs pooled(1-3) ----------------\n",
    "    R = df[df[\"bin\"].isin([1,2,3])][metric].dropna().to_numpy()\n",
    "    B4 = df[df[\"bin\"].isin([4])][metric].dropna().to_numpy()\n",
    "\n",
    "    # Brunner–Munzel one-sided: H1: R > B4 (higher NSE better)\n",
    "    bm = stats.brunnermunzel(R, B4, alternative=\"greater\")\n",
    "\n",
    "    # Effect sizes\n",
    "    delta = cliffs_delta(R, B4)\n",
    "    cles = common_language_effect(R, B4)  # P(R>B4)\n",
    "\n",
    "    # Cluster-boot CI for delta\n",
    "    df_pair = df.copy()\n",
    "    # we’ll compute delta on the fly inside bootstrap\n",
    "    def delta_func(x, y):  # wrapper not used directly\n",
    "        return cliffs_delta(x, y)\n",
    "\n",
    "    def _delta_stat(x, y):  # for bootstrap\n",
    "        return cliffs_delta(x, y)\n",
    "\n",
    "    # Reuse the generic CI helper by passing a small shim:\n",
    "    def stat_func(x, y): return cliffs_delta(x, y)\n",
    "    delta_ci = cluster_bootstrap_ci(\n",
    "        df, \"prediction_huc\", metric, metric, stat_func=stat_func, B=B_boot, seed=202\n",
    "    )\n",
    "\n",
    "    # ---------------- Quantile shifts R vs B4 ----------------\n",
    "    taus = [0.10, 0.25, 0.50, 0.75, 0.90]\n",
    "    q_rows = []\n",
    "    for tau in taus:\n",
    "        dq = np.percentile(R, tau*100) - np.percentile(B4, tau*100)\n",
    "        ci = cluster_bootstrap_ci(\n",
    "            df, \"prediction_huc\", metric, metric, stat_func=quantile_diff_func(tau),\n",
    "            B=B_boot, seed=100+int(tau*1000)\n",
    "        )\n",
    "        q_rows.append({\"tau\": tau, \"diff\": dq, \"ci_low\": ci[0], \"ci_high\": ci[1]})\n",
    "    qshift = pd.DataFrame(q_rows)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"KW(4 bins):\", kw_stat, kw_p)\n",
    "        print(\"Spearman rho (bin vs metric):\", spearman_rho, spearman_p, \"CI:\", rho_ci)\n",
    "        print(\"KW(bins 1-3):\", kw123_stat, kw123_p)\n",
    "        print(\"Brunner–Munzel R>bin4:\", bm.statistic, bm.pvalue)\n",
    "        print(\"Cliff's delta:\", delta, \"CI:\", delta_ci, \"CLES P(R>B4):\", cles)\n",
    "        print(qshift)\n",
    "\n",
    "    results = {\n",
    "        \"kw_4bins\": {\"H\": kw_stat, \"p\": kw_p},\n",
    "        \"spearman_trend\": {\"rho\": spearman_rho, \"p\": spearman_p, \"ci95\": rho_ci},\n",
    "        \"kw_bins123\": {\"H\": kw123_stat, \"p\": kw123_p},\n",
    "        \"dunn_bins123\": dunn_123,  # has BH q-values\n",
    "        \"bm_R_vs_bin4\": {\"stat\": bm.statistic, \"p_one_sided\": bm.pvalue},\n",
    "        \"effect_R_vs_bin4\": {\n",
    "            \"cliffs_delta\": delta, \"cliffs_delta_ci95\": delta_ci, \"cles\": cles\n",
    "        },\n",
    "        \"qshift_R_vs_bin4\": qshift\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KW(4 bins): 2.111993933499434 0.5494901225322859\n",
      "Spearman rho (bin vs metric): -0.03483813682955301 0.48993666553556203 CI: (np.float64(-0.11976938306674517), np.float64(0.056626563823235436))\n",
      "KW(bins 1-3): 1.6046961802038595 0.44827513694883125\n",
      "Brunner–Munzel R>bin4: -0.7825644256125297 0.21719792677786165\n",
      "Cliff's delta: 0.046097599745668516 CI: (np.float64(-0.07487741412354412), np.float64(0.14868704353150547)) CLES P(R>B4): 0.5230487998728343\n",
      "    tau      diff    ci_low   ci_high\n",
      "0  0.10  0.003110 -0.210690  0.124321\n",
      "1  0.25 -0.014237 -0.065481  0.042009\n",
      "2  0.50  0.007601 -0.017800  0.044022\n",
      "3  0.75  0.002787 -0.007537  0.016334\n",
      "4  0.90  0.003528 -0.007880  0.014747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kw_4bins': {'H': np.float64(2.111993933499434),\n",
       "  'p': np.float64(0.5494901225322859)},\n",
       " 'spearman_trend': {'rho': np.float64(-0.03483813682955301),\n",
       "  'p': np.float64(0.48993666553556203),\n",
       "  'ci95': (np.float64(-0.11976938306674517),\n",
       "   np.float64(0.056626563823235436))},\n",
       " 'kw_bins123': {'H': np.float64(1.6046961802038595),\n",
       "  'p': np.float64(0.44827513694883125)},\n",
       " 'dunn_bins123':    g1  g2         z         p      q_BH\n",
       " 0   1   2 -1.251773  0.210653  0.453042\n",
       " 1   1   3 -0.848127  0.396367  0.453042\n",
       " 2   2   3  0.750353  0.453042  0.453042,\n",
       " 'bm_R_vs_bin4': {'stat': np.float64(-0.7825644256125297),\n",
       "  'p_one_sided': np.float64(0.21719792677786165)},\n",
       " 'effect_R_vs_bin4': {'cliffs_delta': np.float64(0.046097599745668516),\n",
       "  'cliffs_delta_ci95': (np.float64(-0.07487741412354412),\n",
       "   np.float64(0.14868704353150547)),\n",
       "  'cles': np.float64(0.5230487998728343)},\n",
       " 'qshift_R_vs_bin4':     tau      diff    ci_low   ci_high\n",
       " 0  0.10  0.003110 -0.210690  0.124321\n",
       " 1  0.25 -0.014237 -0.065481  0.042009\n",
       " 2  0.50  0.007601 -0.017800  0.044022\n",
       " 3  0.75  0.002787 -0.007537  0.016334\n",
       " 4  0.90  0.003528 -0.007880  0.014747}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = '/home/sarth/rootdir/workdir/projects/Paper_Data_Latency/Revised_Statistical_Testing/Figure02'\n",
    "# ,prediction_huc,NSE,F1,no_flow_frac_strict,no_flow_frac_threshold\n",
    "# Choose thresholded definition (Q<1); analyze NSE\n",
    "res = analyze_no_flow_effect(\n",
    "    f\"{csv_path}/continental_with_flow_frac.csv\",\n",
    "    metric=\"NSE\",\n",
    "    noflow_col=\"no_flow_frac_threshold\",  # or \"no_flow_strict\"\n",
    "    bin_size=0.25,\n",
    "    B_boot=1000,\n",
    "    verbose=True\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KW(4 bins): 3.4180091641992476 0.331552778893218\n",
      "Spearman rho (bin vs metric): -0.01825003719433364 0.717656245679341 CI: (np.float64(-0.10164996426903009), np.float64(0.07710401549947192))\n",
      "KW(bins 1-3): 0.7378382904337286 0.6914813177620813\n",
      "Brunner–Munzel R>bin4: -1.3958093631984319 0.09115532279541295\n",
      "Cliff's delta: 0.24142480211081785 CI: (np.float64(-0.016483423300920716), np.float64(0.5258267974284107)) CLES P(R>B4): 0.6207124010554089\n",
      "    tau      diff    ci_low   ci_high\n",
      "0  0.10  0.169384 -0.088020  7.261137\n",
      "1  0.25  0.257701  0.002260  0.464755\n",
      "2  0.50  0.109388  0.016869  0.392400\n",
      "3  0.75  0.005137 -0.026755  0.138015\n",
      "4  0.90 -0.000861 -0.013788  0.064100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kw_4bins': {'H': np.float64(3.4180091641992476),\n",
       "  'p': np.float64(0.331552778893218)},\n",
       " 'spearman_trend': {'rho': np.float64(-0.01825003719433364),\n",
       "  'p': np.float64(0.717656245679341),\n",
       "  'ci95': (np.float64(-0.10164996426903009), np.float64(0.07710401549947192))},\n",
       " 'kw_bins123': {'H': np.float64(0.7378382904337286),\n",
       "  'p': np.float64(0.6914813177620813)},\n",
       " 'dunn_bins123':    g1  g2         z         p      q_BH\n",
       " 0   1   2 -0.858825  0.390437  0.955265\n",
       " 1   1   3 -0.026964  0.978489  0.978489\n",
       " 2   2   3  0.472117  0.636843  0.955265,\n",
       " 'bm_R_vs_bin4': {'stat': np.float64(-1.3958093631984319),\n",
       "  'p_one_sided': np.float64(0.09115532279541295)},\n",
       " 'effect_R_vs_bin4': {'cliffs_delta': np.float64(0.24142480211081785),\n",
       "  'cliffs_delta_ci95': (np.float64(-0.016483423300920716),\n",
       "   np.float64(0.5258267974284107)),\n",
       "  'cles': np.float64(0.6207124010554089)},\n",
       " 'qshift_R_vs_bin4':     tau      diff    ci_low   ci_high\n",
       " 0  0.10  0.169384 -0.088020  7.261137\n",
       " 1  0.25  0.257701  0.002260  0.464755\n",
       " 2  0.50  0.109388  0.016869  0.392400\n",
       " 3  0.75  0.005137 -0.026755  0.138015\n",
       " 4  0.90 -0.000861 -0.013788  0.064100}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = '/home/sarth/rootdir/workdir/projects/Paper_Data_Latency/Revised_Statistical_Testing/Figure02'\n",
    "# ,prediction_huc,NSE,F1,no_flow_frac_strict,no_flow_frac_threshold\n",
    "# Choose thresholded definition (Q<1); analyze NSE\n",
    "res1 = analyze_no_flow_effect(\n",
    "    f\"{csv_path}/continental_with_flow_frac.csv\",\n",
    "    metric=\"NSE\",\n",
    "    noflow_col=\"no_flow_frac_strict\",  # or \"no_flow_strict\"\n",
    "    bin_size=0.25,\n",
    "    B_boot=1000,\n",
    "    verbose=True\n",
    ")\n",
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
