{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Setting Up\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# import xesmf as xe\n",
    "import networkx as nx\n",
    "# import rioxarray as rxr\n",
    "\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import itertools\n",
    "import tqdm\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import torch\n",
    "\n",
    "import configparser\n",
    "cfg = configparser.ConfigParser()\n",
    "cfg.optionxform = str\n",
    "cfg.read('/home/sarth/rootdir/datadir/assets/defaults.ini')\n",
    "cfg = {s: dict(cfg.items(s)) for s in cfg.sections()}\n",
    "PATHS = cfg['PATHS']\n",
    "\n",
    "print(\"Setting up...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Region-Specific: CAMELS-US\n",
    "DIRNAME = '03min_GloFAS_CAMELS-US'\n",
    "SAVE_PATH = os.path.join(PATHS['devp_datasets'], DIRNAME)\n",
    "resolution = 0.05\n",
    "lon_360_180 = lambda x: (x + 180) % 360 - 180 # convert 0-360 to -180-180\n",
    "lon_180_360 = lambda x: x % 360 # convert -180-180 to 0-360\n",
    "region_bounds = {\n",
    "    'minx': -130,\n",
    "    'miny': 20,\n",
    "    'maxx': -65,\n",
    "    'maxy': 50\n",
    "}\n",
    "camels_attributes_graph = pd.read_csv(os.path.join(SAVE_PATH, 'graph_attributes.csv'), index_col=0)\n",
    "camels_attributes_graph.index = camels_attributes_graph.index.map(lambda x: str(x).zfill(8))\n",
    "camels_attributes_graph['huc_02'] = camels_attributes_graph['huc_02'].map(lambda x: str(x).zfill(2))\n",
    "camels_graph = camels_attributes_graph.copy()\n",
    "camels_graph = camels_graph[camels_graph['area_percent_difference'] < 10]\n",
    "camels_graph = camels_graph[camels_graph['num_nodes'] > 1]\n",
    "print(f\"Number of CAMELS-US catmt's: {len(camels_graph)}\")\n",
    "del camels_attributes_graph\n",
    "\n",
    "region_shp = gpd.read_file(os.path.join(PATHS['watershed-boundary-dataset'], 'huc02', 'shapefile.shp'), crs = 'epsg:4326')\n",
    "all_watersheds = region_shp.copy()\n",
    "all_watersheds = all_watersheds.rename(columns={'huc2': 'watershed'})\n",
    "all_watersheds['huc_02'] = all_watersheds['watershed'].map(lambda x: x.split('_')[0])\n",
    "\n",
    "temp = gpd.read_file(os.path.join(PATHS['CAMELS'], 'CAMELS-US', 'HCDN_nhru_final_671.shp'), crs = 'epsg:4326')\n",
    "temp = temp[['hru_id', 'geometry']]\n",
    "temp['hru_id'] = temp['hru_id'].map(lambda x: str(x).zfill(8))\n",
    "temp = temp.set_index('hru_id')\n",
    "\n",
    "all_catchments = camels_graph.merge(temp, left_index=True, right_index=True, how='left')\n",
    "all_catchments = all_catchments[['huc_02', 'gauge_lon', 'gauge_lat', 'area_geospa_fabric', 'geometry', 'snapped_lon', 'snapped_lat']]\n",
    "all_catchments = gpd.GeoDataFrame(all_catchments, crs='epsg:4326', geometry='geometry')\n",
    "all_catchments = all_catchments.reset_index()\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats, signal\n",
    "\n",
    "def _mask_valid(pred, true):\n",
    "    mask = ~np.isnan(true) & ~np.isnan(pred)\n",
    "    pred = pred[mask]\n",
    "    true = true[mask]\n",
    "    pred[pred < 0] = 0\n",
    "    true[true < 0] = 0\n",
    "    return pred, true\n",
    "\n",
    "def RMSE(pred, true):\n",
    "    pred, true = _mask_valid(pred, true)\n",
    "    return np.sqrt(np.mean((true - pred)**2))\n",
    "\n",
    "def pearsonr(pred, true):\n",
    "    pred, true = _mask_valid(pred, true)\n",
    "    r, _ = stats.pearsonr(true, pred)\n",
    "    return r\n",
    "\n",
    "def NSE(pred, true):\n",
    "    pred, true = _mask_valid(pred, true)\n",
    "    return 1 - np.sum((true - pred)**2) / np.sum((true - np.mean(true))**2)\n",
    "\n",
    "def KGE(pred, true):\n",
    "    pred, true = _mask_valid(pred, true)\n",
    "    r = pearsonr(pred, true)\n",
    "    alpha = np.std(pred) / np.std(true)\n",
    "    beta = np.mean(pred) / np.mean(true)\n",
    "    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)\n",
    "\n",
    "def PBIAS(pred, true):\n",
    "    pred, true = _mask_valid(pred, true)\n",
    "    return np.sum(true - pred) / np.sum(true) * 100\n",
    "\n",
    "def alpha_NSE(pred, true):\n",
    "    pred, true = _mask_valid(pred, true)\n",
    "    return np.std(pred) / np.std(true)\n",
    "\n",
    "def beta_NSE(pred, true):\n",
    "    pred, true = _mask_valid(pred, true)\n",
    "    return (np.mean(pred) - np.mean(true)) / np.std(true)\n",
    "\n",
    "def _get_fdc(data):\n",
    "    data = np.sort(data)[::-1]\n",
    "    return data\n",
    "\n",
    "def fdc_fms(pred, true, lower = 0.2, upper = 0.7):\n",
    "    pred, true = _mask_valid(pred, true)\n",
    "\n",
    "    sim = _get_fdc(pred)\n",
    "    obs = _get_fdc(true)\n",
    "    sim[sim <= 0] = 1e-6\n",
    "    obs[obs <= 0] = 1e-6\n",
    "\n",
    "    qsm_lower = np.log(sim[np.round(lower * len(sim)).astype(int)])\n",
    "    qsm_upper = np.log(sim[np.round(upper * len(sim)).astype(int)])\n",
    "    qom_lower = np.log(obs[np.round(lower * len(obs)).astype(int)])\n",
    "    qom_upper = np.log(obs[np.round(upper * len(obs)).astype(int)])\n",
    "\n",
    "    fms = ((qsm_lower - qsm_upper) - (qom_lower - qom_upper)) / (qom_lower - qom_upper + 1e-6)\n",
    "\n",
    "    return fms * 100\n",
    "\n",
    "def fdc_fhv(pred, true, h = 0.02):\n",
    "    pred, true = _mask_valid(pred, true)\n",
    "\n",
    "    sim = _get_fdc(pred)\n",
    "    obs = _get_fdc(true)\n",
    "\n",
    "    obs = obs[:np.round(h * len(obs)).astype(int)]\n",
    "    sim = sim[:np.round(h * len(sim)).astype(int)]\n",
    "\n",
    "    fhv = np.sum(sim - obs) / np.sum(obs)\n",
    "\n",
    "    return fhv * 100\n",
    "\n",
    "def fdc_flv(pred, true, l = 0.3):\n",
    "    pred, true = _mask_valid(pred, true)\n",
    "\n",
    "    sim = _get_fdc(pred)\n",
    "    obs = _get_fdc(true)\n",
    "    sim[sim <= 0] = 1e-6\n",
    "    obs[obs <= 0] = 1e-6\n",
    "\n",
    "    obs = obs[-np.round(l * len(obs)).astype(int):]\n",
    "    sim = sim[-np.round(l * len(sim)).astype(int):]\n",
    "\n",
    "    # transform values to log scale\n",
    "    obs = np.log(obs)\n",
    "    sim = np.log(sim)\n",
    "\n",
    "    # calculate flv part by part\n",
    "    qsl = np.sum(sim - sim.min())\n",
    "    qol = np.sum(obs - obs.min())\n",
    "\n",
    "    flv = -1 * (qsl - qol) / (qol + 1e-6)\n",
    "\n",
    "    return flv * 100\n",
    "\n",
    "def mean_peak_timing(pred, true, window = 3):\n",
    "    pred, true = _mask_valid(pred, true)\n",
    "\n",
    "    peaks, _ = signal.find_peaks(true, distance=2*window, prominence=np.std(true))\n",
    "\n",
    "    # pred_idx_lst = []\n",
    "    timing_error_lst = []\n",
    "    for idx in peaks:\n",
    "        if (pred[idx] > pred[idx - 1]) and (pred[idx] > pred[idx + 1]):\n",
    "            peak_pred = pred[idx]\n",
    "            peak_pred_idx = idx\n",
    "        else:\n",
    "            peak_pred_idx = np.argmax(pred[max(idx - window,0):idx + window + 1]) + max(idx - window,0)\n",
    "            peak_pred = pred[peak_pred_idx]\n",
    "        # pred_idx_lst.append(peak_pred_idx)\n",
    "    \n",
    "        peak_true = true[idx]\n",
    "        timing_error = np.abs(peak_pred_idx - idx) \n",
    "        timing_error_lst.append(timing_error)\n",
    "    \n",
    "    mean_timing_error = np.mean(timing_error_lst) if len(timing_error_lst) > 0 else np.nan\n",
    "\n",
    "    return mean_timing_error\n",
    "\n",
    "def missed_peaks(pred, true, window = 3, threshold = 80):\n",
    "    pred, true = _mask_valid(pred, true)\n",
    "\n",
    "    peaks_obs_times, _ = signal.find_peaks(true, distance=2*window, height = np.percentile(true, threshold))\n",
    "    peaks_sim_times, _ = signal.find_peaks(pred, distance=2*window, height = np.percentile(pred, threshold))\n",
    "    \n",
    "    missed_events = 0\n",
    "    for idx in peaks_obs_times:\n",
    "        nearby_peak_sim_index = np.where(np.abs(peaks_sim_times - idx) <= window)[0]\n",
    "        if len(nearby_peak_sim_index) == 0:\n",
    "            missed_events += 1\n",
    "            # print(idx)\n",
    "    \n",
    "    missed_peak_values = (missed_events / len(peaks_obs_times)) * 100 if len(peaks_obs_times) > 0 else np.nan\n",
    "\n",
    "    return missed_peak_values\n",
    "\n",
    "def F1_score_of_capturing_peaks(pred, true, window = 3, threshold = 80):\n",
    "    pred, true = _mask_valid(pred, true)\n",
    "\n",
    "    peaks_obs_times, _ = signal.find_peaks(true, distance=2*window, height = np.percentile(true, threshold))\n",
    "    peaks_sim_times, _ = signal.find_peaks(pred, distance=2*window, height = np.percentile(pred, threshold))\n",
    "    \n",
    "    true_positive_peaks = 0 # peak in obs and nearby in sim\n",
    "    true_negative_peaks = 0 # no peak in obs and sim\n",
    "    false_positive_peaks = 0 # peak in sim but not nearby in obs\n",
    "    false_negative_peaks = 0 # peak in obs but not nearby in sim\n",
    "\n",
    "    for idx in peaks_obs_times:\n",
    "        nearby_peak_sim_index = np.where(np.abs(peaks_sim_times - idx) <= window)[0]\n",
    "        if len(nearby_peak_sim_index) > 0:\n",
    "            true_positive_peaks += 1\n",
    "        else:\n",
    "            false_negative_peaks += 1\n",
    "    \n",
    "    for idx in peaks_sim_times:\n",
    "        nearby_peak_obs_index = np.where(np.abs(peaks_obs_times - idx) <= window)[0]\n",
    "        if len(nearby_peak_obs_index) == 0:\n",
    "            false_positive_peaks += 1\n",
    "\n",
    "    precision = true_positive_peaks / (true_positive_peaks + false_positive_peaks) if (true_positive_peaks + false_positive_peaks) > 0 else np.nan\n",
    "    recall = true_positive_peaks / (true_positive_peaks + false_negative_peaks) if (true_positive_peaks + false_negative_peaks) > 0 else np.nan\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else np.nan\n",
    "\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "def compute_metrics_ds(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Compute a set of metrics for each lead time and catchment.\n",
    "    y_pred and y_true: numpy arrays of shape (time_idx, lead_time, catmt_idx)\n",
    "    \n",
    "    Returns:\n",
    "      xr.Dataset with dims (\"time_idx\", \"lead_time\", \"catmt_idx\") and variables:\n",
    "          - RMSE, PearsonR, NSE, KGE, PBIAS, alpha_NSE, beta_NSE,\n",
    "            FDC_FMS, FDC_FHV, FDC_FLV, mean_peak_timing, missed_peaks, F1_score\n",
    "          - Also includes raw y_pred and y_true.\n",
    "    \"\"\"\n",
    "\n",
    "    time_steps, num_leadtimes, num_catmt = y_pred.shape\n",
    "\n",
    "    # Prepare arrays to hold computed metrics [lead_time, catmt_idx]\n",
    "    rmse_arr            = np.empty((num_leadtimes, num_catmt))\n",
    "    pearson_arr         = np.empty((num_leadtimes, num_catmt))\n",
    "    nse_arr             = np.empty((num_leadtimes, num_catmt))\n",
    "    kge_arr             = np.empty((num_leadtimes, num_catmt))\n",
    "    pbias_arr           = np.empty((num_leadtimes, num_catmt))\n",
    "    alpha_nse_arr       = np.empty((num_leadtimes, num_catmt))\n",
    "    beta_nse_arr        = np.empty((num_leadtimes, num_catmt))\n",
    "    fdc_fms_arr         = np.empty((num_leadtimes, num_catmt))\n",
    "    fdc_fhv_arr         = np.empty((num_leadtimes, num_catmt))\n",
    "    fdc_flv_arr         = np.empty((num_leadtimes, num_catmt))\n",
    "    mean_peak_timing_arr= np.empty((num_leadtimes, num_catmt))\n",
    "    missed_peaks_arr    = np.empty((num_leadtimes, num_catmt))\n",
    "    f1_score_arr        = np.empty((num_leadtimes, num_catmt))\n",
    "\n",
    "    # Loop over lead times and catchments, computing metrics from the time series\n",
    "    for lt in range(num_leadtimes):\n",
    "        for cat in range(num_catmt):\n",
    "            pred = y_pred[:, lt, cat]\n",
    "            true = y_true[:, lt, cat]\n",
    "            rmse_arr[lt, cat]             = RMSE(pred, true)\n",
    "            pearson_arr[lt, cat]          = pearsonr(pred, true)\n",
    "            nse_arr[lt, cat]              = NSE(pred, true)\n",
    "            kge_arr[lt, cat]              = KGE(pred, true)\n",
    "            pbias_arr[lt, cat]            = PBIAS(pred, true)\n",
    "            alpha_nse_arr[lt, cat]        = alpha_NSE(pred, true)\n",
    "            beta_nse_arr[lt, cat]         = beta_NSE(pred, true)\n",
    "            fdc_fms_arr[lt, cat]          = fdc_fms(pred, true)\n",
    "            fdc_fhv_arr[lt, cat]          = fdc_fhv(pred, true)\n",
    "            fdc_flv_arr[lt, cat]          = fdc_flv(pred, true)\n",
    "            mean_peak_timing_arr[lt, cat] = mean_peak_timing(pred, true)\n",
    "            missed_peaks_arr[lt, cat]     = missed_peaks(pred, true)\n",
    "            f1_score_arr[lt, cat]         = F1_score_of_capturing_peaks(pred, true)\n",
    "\n",
    "    # Create coordinates\n",
    "    lead_times = np.arange(num_leadtimes)\n",
    "    catmt_idx  = np.arange(num_catmt)\n",
    "    time_idx   = np.arange(time_steps)\n",
    "\n",
    "    ds = xr.Dataset(\n",
    "        {\n",
    "            \"RMSE\":             ((\"lead_time\", \"catmt_idx\"), rmse_arr),\n",
    "            \"PearsonR\":         ((\"lead_time\", \"catmt_idx\"), pearson_arr),\n",
    "            \"NSE\":              ((\"lead_time\", \"catmt_idx\"), nse_arr),\n",
    "            \"KGE\":              ((\"lead_time\", \"catmt_idx\"), kge_arr),\n",
    "            \"PBIAS\":            ((\"lead_time\", \"catmt_idx\"), pbias_arr),\n",
    "            \"alpha_NSE\":        ((\"lead_time\", \"catmt_idx\"), alpha_nse_arr),\n",
    "            \"beta_NSE\":         ((\"lead_time\", \"catmt_idx\"), beta_nse_arr),\n",
    "            \"FDC_FMS\":          ((\"lead_time\", \"catmt_idx\"), fdc_fms_arr),\n",
    "            \"FDC_FHV\":          ((\"lead_time\", \"catmt_idx\"), fdc_fhv_arr),\n",
    "            \"FDC_FLV\":          ((\"lead_time\", \"catmt_idx\"), fdc_flv_arr),\n",
    "            \"mean_peak_timing\": ((\"lead_time\", \"catmt_idx\"), mean_peak_timing_arr),\n",
    "            \"missed_peaks\":     ((\"lead_time\", \"catmt_idx\"), missed_peaks_arr),\n",
    "            \"F1_score\":         ((\"lead_time\", \"catmt_idx\"), f1_score_arr),\n",
    "            \"y_pred\":           ((\"time_idx\", \"lead_time\", \"catmt_idx\"), y_pred),\n",
    "            \"y_true\":           ((\"time_idx\", \"lead_time\", \"catmt_idx\"), y_true)\n",
    "        },\n",
    "        coords={\n",
    "            \"time_idx\": time_idx,\n",
    "            \"lead_time\": lead_times,\n",
    "            \"catmt_idx\": catmt_idx\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return ds\n",
    "\n",
    "# # Example usage:\n",
    "# metrics_ds = compute_metrics_ds(y_pred, y_true)\n",
    "# metrics_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_camelsus = lambda x: os.path.join('/home/sarth/rootdir/workdir/projects/Paper_Data_Latency/Figure04', f'US_v{x}.nc')\n",
    "\n",
    "filepath_camelsind = lambda x: os.path.join('/home/sarth/rootdir/workdir/projects/Paper_Data_Latency/Figure04', f'IND_v{x}.nc')\n",
    "\n",
    "filepath_hysets = lambda x: os.path.join('/home/sarth/rootdir/workdir/projects/Paper_Data_Latency/hysets', f'US_hysets_v{x}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 3\n",
    "ds_camelsus = xr.open_dataset(filepath_camelsus(V))\n",
    "ds_camelsind = xr.open_dataset(filepath_camelsind(V))\n",
    "ds_hysets = xr.open_dataset(filepath_hysets(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_hysets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_cdf(ds, varname, lead_times=None, clip_min_max=[-1, 1], cmap_name = 'viridis'):\n",
    "#     grid = np.linspace(clip_min_max[0], clip_min_max[1], 100)\n",
    "#     all_cdfs = {}\n",
    "#     if lead_times is None:\n",
    "#         lead_times = ds['lead_time'].values\n",
    "#         lead_times += 1\n",
    "    \n",
    "#     if 'F1' in varname:\n",
    "#         varname_label = 'F1 score of peaks captured'\n",
    "#         varname_legend = 'F1'\n",
    "#     else:\n",
    "#         varname_label = varname\n",
    "#         varname_legend = varname\n",
    "\n",
    "#     for lt in lead_times:\n",
    "#         data = ds[varname].sel(lead_time=lt-1).values.flatten()\n",
    "#         data = data[~np.isnan(data)]\n",
    "#         cdf_values = np.array([np.mean(data <= val) for val in grid])\n",
    "#         all_cdfs[lt] = cdf_values\n",
    "\n",
    "#     cmap = cm.get_cmap(cmap_name, len(lead_times))\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(8, 6))\n",
    "#     ax.set_facecolor('whitesmoke')\n",
    "#     for i, lt in enumerate(lead_times):\n",
    "#         median_lt = np.nanmedian(ds[varname].sel(lead_time=lt-1).values.flatten())\n",
    "#         plt.plot(grid, all_cdfs[lt], label=f't+{lt}: {median_lt:.2f}', color=cmap(i))\n",
    "\n",
    "#     plt.axhline(0.5, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "#     # Create a secondary y-axis for PDF\n",
    "#     ax2 = ax.twinx()\n",
    "#     ax2.set_ylabel('PDF')\n",
    "\n",
    "#     bar_width = grid[1] - grid[0]\n",
    "#     all_pdfs = {}\n",
    "#     for i, lt in enumerate(lead_times):\n",
    "#         pdf_values = np.gradient(all_cdfs[lt], grid)\n",
    "#         all_pdfs[lt] = pdf_values\n",
    "#         ax2.bar(grid, pdf_values, width=bar_width, alpha=0.1, color=cmap(i), align='center')\n",
    "\n",
    "#     ax.set_xlabel(varname_label)\n",
    "#     ax.set_ylabel('CDF')\n",
    "#     ax.legend(loc = 'upper left', title=f'Lead Time (Median {varname_legend})', fontsize=10)\n",
    "#     ax.grid()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "def plot_cdf(ds, varname, lead_times=None, clip_min_max=[-1, 1], cmap_name='viridis', high_res = False):\n",
    "    grid = np.linspace(clip_min_max[0], clip_min_max[1], 100)\n",
    "    all_cdfs = {}\n",
    "    if lead_times is None:\n",
    "        lead_times = ds['lead_time'].values\n",
    "        lead_times += 1\n",
    "\n",
    "    if 'F1' in varname:\n",
    "        varname_label = 'F1 score of peaks captured'\n",
    "        varname_legend = 'F1'\n",
    "    else:\n",
    "        varname_label = varname\n",
    "        varname_legend = varname\n",
    "\n",
    "    # Extract the baseline data (first lead time) for KS comparison\n",
    "    baseline_data = ds[varname].sel(lead_time=lead_times[0]-1).values.flatten()\n",
    "    baseline_data = baseline_data[~np.isnan(baseline_data)]\n",
    "    \n",
    "    # Compute CDFs for each lead time\n",
    "    for lt in lead_times:\n",
    "        data = ds[varname].sel(lead_time=lt-1).values.flatten()\n",
    "        data = data[~np.isnan(data)]\n",
    "        cdf_values = np.array([np.mean(data <= val) for val in grid])\n",
    "        all_cdfs[lt] = cdf_values\n",
    "\n",
    "    cmap = cm.get_cmap(cmap_name, len(lead_times))\n",
    "    if high_res:\n",
    "        fig, ax = plt.subplots(figsize=(16, 12), dpi = 600)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    # ax.set_facecolor('whitesmoke')\n",
    "    for i, lt in enumerate(lead_times):\n",
    "        data = ds[varname].sel(lead_time=lt-1).values.flatten()\n",
    "        data = data[~np.isnan(data)]\n",
    "        # Compute the KS statistic comparing current and baseline data\n",
    "        ks_stat, _ = stats.ks_2samp(data, baseline_data)\n",
    "        median_lt = np.nanmedian(data)\n",
    "        plt.plot(grid, all_cdfs[lt],\n",
    "                 label=f't+{lt:02d} day: {median_lt:.2f} ({ks_stat:.2f})',\n",
    "                 color=cmap(i))\n",
    "    \n",
    "    plt.axhline(0.5, color='black', linestyle='--', linewidth=1, alpha=0.33)\n",
    "\n",
    "    # Create a secondary y-axis for PDF\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.set_ylabel('PDF')\n",
    "\n",
    "    bar_width = grid[1] - grid[0]\n",
    "    for i, lt in enumerate(lead_times):\n",
    "        pdf_values = np.gradient(all_cdfs[lt], grid)\n",
    "        ax2.bar(grid, pdf_values, width=bar_width, alpha=0.1, color=cmap(i), align='center')\n",
    "\n",
    "\n",
    "    # Compute summary statistics for each lead time using the chosen metric\n",
    "    lead_time_vals = ds['lead_time'].values + 1\n",
    "    medians = []\n",
    "    q25 = []\n",
    "    q75 = []\n",
    "    for lt in lead_time_vals:\n",
    "        data = ds['NSE'].sel(lead_time=lt-1).values.flatten()\n",
    "        data = data[~np.isnan(data)]\n",
    "        medians.append(np.nanmedian(data))\n",
    "        q25.append(np.nanpercentile(data, 25))\n",
    "        q75.append(np.nanpercentile(data, 75))\n",
    "        \n",
    "    # Create an inset axes and plot line with error bars (25th-75th percentile)\n",
    "    axins = inset_axes(\n",
    "        ax,\n",
    "        width=\"40%\",\n",
    "        height=\"20%\",\n",
    "        bbox_to_anchor=(0.05, -0.05, 0.8, 0.75),  # left, bottom, width, height\n",
    "        bbox_transform=ax.transAxes,\n",
    "        loc=2\n",
    "    )\n",
    "    axins.set_facecolor('whitesmoke')\n",
    "    yerr_lower = np.array(medians) - np.array(q25)\n",
    "    yerr_upper = np.array(q75) - np.array(medians)\n",
    "    axins.errorbar(lead_time_vals, medians, yerr=[yerr_lower, yerr_upper],\n",
    "                   fmt='-o', color='black', capsize=3)\n",
    "    # axins.set_xlabel(\"Lead Time\", fontsize=8)\n",
    "    axins.set_ylabel('NSE', fontsize=12, labelpad=0)\n",
    "    # axins.set_title(varname_label, fontsize=8)\n",
    "    axins.tick_params(axis='both', which='major', labelsize=8)\n",
    "    # Rotate y-ticks for better readability\n",
    "    axins.yaxis.set_tick_params(rotation=60)\n",
    "    axins.set_xticks(lead_time_vals)\n",
    "    axins.set_xticklabels([f't+{int(lt)}' if lt in [1, 3, 5, 7, 10] else \"\" for lt in lead_time_vals], fontsize=8)\n",
    "\n",
    "    axins.grid(True)\n",
    "\n",
    "    ax.set_xlabel(varname_label)\n",
    "    ax.set_ylabel('CDF')\n",
    "    ax.legend(loc='upper left', title=f'Lead Time: Median {varname_legend} (KS statistic)', fontsize=10)\n",
    "    ax.grid(alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_cdf(ds_camelsus, 'NSE', lead_times=[1,3,5,7,10], clip_min_max=[-1, 1],cmap_name='plasma', high_res = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf(ds_camelsus, 'F1_score', lead_times=[1,3,5,7,10], clip_min_max=[0, 1],cmap_name='plasma', high_res = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_cdf(ds_hysets, 'NSE', lead_times=[1,3,5,7,10], clip_min_max=[-1, 1],cmap_name='plasma', high_res = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf(ds_hysets, 'F1_score', lead_times=[1,3,5,7,10], clip_min_max=[0, 1],cmap_name='plasma', high_res = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_cdf(ds_camelsind, 'NSE', lead_times=[1,3,5,7,10], clip_min_max=[-1, 1],cmap_name='plasma', high_res = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf(ds_camelsind, 'F1_score', lead_times=[1,3,5,7,10], clip_min_max=[0, 1],cmap_name='plasma', high_res = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
